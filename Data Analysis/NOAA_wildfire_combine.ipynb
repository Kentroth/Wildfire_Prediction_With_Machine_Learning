{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from config import token\n",
    "import sqlite3\n",
    "import warnings\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Count forOutputs/US_data/temp_2020_01.csv: 10432\n",
      "Response Count forOutputs/US_data/temp_2020_02.csv: 10468\n",
      "Response Count forOutputs/US_data/temp_2020_03.csv: 10071\n",
      "Response Count forOutputs/US_data/temp_2020_04.csv: 10110\n",
      "Response Count forOutputs/US_data/temp_2020_05.csv: 10073\n",
      "Response Count forOutputs/US_data/temp_2020_06.csv: 10079\n",
      "Response Count forOutputs/US_data/temp_2020_07.csv: 10068\n",
      "Response Count forOutputs/US_data/temp_2020_08.csv: 10046\n",
      "Response Count forOutputs/US_data/temp_2020_09.csv: 10074\n",
      "Error: 503\n",
      "Response Count forOutputs/US_data/temp_2020_10.csv: 4000\n",
      "Response Count forOutputs/US_data/temp_2020_11.csv: 9967\n",
      "Response Count forOutputs/US_data/temp_2020_12.csv: 9980\n"
     ]
    }
   ],
   "source": [
    "def fetch_save_data(url, headers, params, csv_filename):\n",
    "    results = []\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            df = pd.DataFrame(data['results'])\n",
    "            results.append(df)\n",
    "            \n",
    "            if len(df) < params['limit']:\n",
    "                break\n",
    "            \n",
    "            params['offset'] += params['limit']\n",
    "        else:\n",
    "            print(\"Error:\", response.status_code)\n",
    "            break\n",
    "    \n",
    "    df_name = pd.concat(results, ignore_index=True)\n",
    "    response_count = df_name.shape[0]\n",
    "    print(f\"Response Count for{csv_filename}: {response_count}\")\n",
    "    \n",
    "    df_name.to_csv(csv_filename, index=False)\n",
    "    return df_name\n",
    "\n",
    "# Define the API endpoint URL, headers, pagination parameters, and base CSV filename\n",
    "api_url = \"https://www.ncei.noaa.gov/cdo-web/api/v2/data\"\n",
    "api_headers = {\"token\": token}\n",
    "limit = 1000\n",
    "offset= 0\n",
    "datasetid = 'GSOM'\n",
    "datatypeids = ['TMAX','TAVG','TMIN'] #'TMAX','TAVG','TMIN' 'PRCP'\n",
    "locationid = 'FIPS:US'  # US FIPS code, california is 06\n",
    "\n",
    "# Define the start and end dates for the range of months you want to fetch\n",
    "start_year = 2020\n",
    "start_month =1\n",
    "end_year = 2020\n",
    "end_month = 12  # Adjust this based on how many months you want to fetch\n",
    "\n",
    "# Specify the number of designated iterations (months)\n",
    "iterations = end_month - start_month + 1\n",
    "\n",
    "# Loop through the specified range of months\n",
    "for _ in range(iterations):\n",
    "    start_date = f\"{start_year}-{start_month:02d}-01\"\n",
    "    end_day = calendar.monthrange(start_year, start_month)[1]\n",
    "    end_date = f\"{start_year}-{start_month:02d}-{end_day:02d}\"\n",
    "    \n",
    "    api_params = {\n",
    "        'offset': offset,\n",
    "        'datasetid': datasetid,\n",
    "        'startdate': start_date,\n",
    "        'enddate': end_date,\n",
    "        'locationid': locationid,\n",
    "        'limit': limit,\n",
    "        'datatypeid': ','.join(datatypeids)\n",
    "    }\n",
    "    \n",
    "    output_csv_filename = f\"Outputs/US_data/temp_{start_year}_{start_month:02d}.csv\" #change between temp and prcp and others\n",
    "    \n",
    "    # Call the function with the defined values\n",
    "    temp_df = fetch_save_data(api_url, api_headers, api_params, output_csv_filename)\n",
    "    \n",
    "    # Increment the start_month and start_year for the next iteration\n",
    "    start_month += 1\n",
    "    if start_month > 12:\n",
    "        start_month = 1\n",
    "        start_year += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>datatype</th>\n",
       "      <th>station</th>\n",
       "      <th>attributes</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01T00:00:00</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>GHCND:AQW00061705</td>\n",
       "      <td>,,,W</td>\n",
       "      <td>390.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01T00:00:00</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>GHCND:CA001018611</td>\n",
       "      <td>,,,C</td>\n",
       "      <td>117.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01T00:00:00</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>GHCND:CA001135126</td>\n",
       "      <td>,,,C</td>\n",
       "      <td>67.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01T00:00:00</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>GHCND:CA005020881</td>\n",
       "      <td>,,,C</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01T00:00:00</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>GHCND:CA006020559</td>\n",
       "      <td>,,,C</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date datatype            station attributes  value\n",
       "0  2018-01-01T00:00:00     PRCP  GHCND:AQW00061705       ,,,W  390.3\n",
       "1  2018-01-01T00:00:00     PRCP  GHCND:CA001018611       ,,,C  117.6\n",
       "2  2018-01-01T00:00:00     PRCP  GHCND:CA001135126       ,,,C   67.6\n",
       "3  2018-01-01T00:00:00     PRCP  GHCND:CA005020881       ,,,C    3.6\n",
       "4  2018-01-01T00:00:00     PRCP  GHCND:CA006020559       ,,,C   21.6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine all daily CSV files into a single DataFrame\n",
    "combined_df = pd.concat([pd.read_csv(os.path.join(\"Outputs/US_data\", file)) for file in os.listdir(\"Outputs/US_data\")])\n",
    "\n",
    "# Save the combined DataFrame as a CSV file\n",
    "combined_csv_filename = 'Outputs/combined_data.csv'\n",
    "combined_df.to_csv(combined_csv_filename, index=False)\n",
    "\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(617457, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_save_data(url, headers, params, csv_filename):\n",
    "    results = []\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            df = pd.DataFrame(data['results'])\n",
    "            results.append(df)\n",
    "            \n",
    "            if len(df) < params['limit']:\n",
    "                break\n",
    "            \n",
    "            params['offset'] += params['limit']\n",
    "        else:\n",
    "            print(\"Error:\", response.status_code)\n",
    "            break\n",
    "    \n",
    "    df_name = pd.concat(results, ignore_index=True)\n",
    "    response_count = df_name.shape[0]\n",
    "    print(f\"Response Count: {response_count}\")\n",
    "    \n",
    "    df_name.to_csv(csv_filename, index=False)\n",
    "    return df_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the API endpoint URL, headers, pagination parameters, and CSV filename\n",
    "api_url = \"https://www.ncei.noaa.gov/cdo-web/api/v2/stations\"\n",
    "api_headers = {\"token\": token}\n",
    "api_params = {'offset': 0, \n",
    "              'limit': 1000,\n",
    "              'locationid': 'FIPS:US' #CA code is 06\n",
    "             }\n",
    "output_csv_filename = 'Outputs/full_station_list.csv'\n",
    "\n",
    "# Call the function with the defined values\n",
    "stations_df = fetch_save_data(api_url, api_headers, api_params, output_csv_filename)\n",
    "\n",
    "stations_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the API endpoint URL, headers, pagination parameters, and CSV filename\n",
    "api_url = \"https://www.ncei.noaa.gov/cdo-web/api/v2/data\"\n",
    "api_headers = {\"token\": token}\n",
    "\n",
    "# Set up pagination parameters\n",
    "offset = 0\n",
    "limit = 1000\n",
    "datasetid = 'GSOM'\n",
    "datatypeids = ['TAVG','TMIN','TMAX','PRCP']\n",
    "startdate = '2018-01-01'\n",
    "enddate = '2018-01-31'\n",
    "locationid = 'FIPS:US'  # California FIPS code\n",
    "\n",
    "api_params = {\n",
    "    'datasetid': datasetid,\n",
    "    'startdate': startdate,\n",
    "    'locationid': locationid,\n",
    "    'enddate': enddate,\n",
    "    'offset': offset,\n",
    "    'limit': limit\n",
    "}\n",
    "\n",
    "# Combine datatypeids into a comma-separated string\n",
    "api_params['datatypeid'] = ','.join(datatypeids)\n",
    "\n",
    "output_csv_filename = 'f(Outputs/US_data/prcp_{year}_{month}.csv)\n",
    "\n",
    "# Call the function with the defined values\n",
    "temp_df = fetch_save_data(api_url, api_headers, api_params, output_csv_filename)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "temp_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>station</th>\n",
       "      <th>TMAX</th>\n",
       "      <th>TMIN</th>\n",
       "      <th>TAVG</th>\n",
       "      <th>PRCP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01T00:00:00</td>\n",
       "      <td>GHCND:AQW00061705</td>\n",
       "      <td>30.58</td>\n",
       "      <td>25.12</td>\n",
       "      <td>27.85</td>\n",
       "      <td>390.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01T00:00:00</td>\n",
       "      <td>GHCND:CA001018611</td>\n",
       "      <td>7.98</td>\n",
       "      <td>4.63</td>\n",
       "      <td>6.30</td>\n",
       "      <td>117.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01T00:00:00</td>\n",
       "      <td>GHCND:CA001135126</td>\n",
       "      <td>1.10</td>\n",
       "      <td>-5.44</td>\n",
       "      <td>-2.17</td>\n",
       "      <td>67.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01T00:00:00</td>\n",
       "      <td>GHCND:CA005020881</td>\n",
       "      <td>-8.93</td>\n",
       "      <td>-17.87</td>\n",
       "      <td>-13.40</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01T00:00:00</td>\n",
       "      <td>GHCND:CA006020559</td>\n",
       "      <td>-8.50</td>\n",
       "      <td>-19.77</td>\n",
       "      <td>-14.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-01-01T00:00:00</td>\n",
       "      <td>GHCND:CQC00914080</td>\n",
       "      <td>27.69</td>\n",
       "      <td>23.23</td>\n",
       "      <td>25.46</td>\n",
       "      <td>273.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-01-01T00:00:00</td>\n",
       "      <td>GHCND:CQC00914801</td>\n",
       "      <td>28.95</td>\n",
       "      <td>24.59</td>\n",
       "      <td>26.77</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-01-01T00:00:00</td>\n",
       "      <td>GHCND:CQC00914855</td>\n",
       "      <td>30.82</td>\n",
       "      <td>24.71</td>\n",
       "      <td>27.77</td>\n",
       "      <td>59.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-01-01T00:00:00</td>\n",
       "      <td>GHCND:GQW00041415</td>\n",
       "      <td>31.10</td>\n",
       "      <td>24.99</td>\n",
       "      <td>28.05</td>\n",
       "      <td>23.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2018-01-01T00:00:00</td>\n",
       "      <td>GHCND:RQC00662801</td>\n",
       "      <td>29.05</td>\n",
       "      <td>20.31</td>\n",
       "      <td>24.68</td>\n",
       "      <td>108.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   date            station   TMAX   TMIN   TAVG   PRCP\n",
       "0   2018-01-01T00:00:00  GHCND:AQW00061705  30.58  25.12  27.85  390.3\n",
       "1   2018-01-01T00:00:00  GHCND:CA001018611   7.98   4.63   6.30  117.6\n",
       "2   2018-01-01T00:00:00  GHCND:CA001135126   1.10  -5.44  -2.17   67.6\n",
       "3   2018-01-01T00:00:00  GHCND:CA005020881  -8.93 -17.87 -13.40    3.6\n",
       "4   2018-01-01T00:00:00  GHCND:CA006020559  -8.50 -19.77 -14.14   21.6\n",
       "5   2018-01-01T00:00:00  GHCND:CQC00914080  27.69  23.23  25.46  273.8\n",
       "6   2018-01-01T00:00:00  GHCND:CQC00914801  28.95  24.59  26.77   57.0\n",
       "7   2018-01-01T00:00:00  GHCND:CQC00914855  30.82  24.71  27.77   59.1\n",
       "9   2018-01-01T00:00:00  GHCND:GQW00041415  31.10  24.99  28.05   23.8\n",
       "25  2018-01-01T00:00:00  GHCND:RQC00662801  29.05  20.31  24.68  108.1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data from your CSV or Excel file\n",
    "file_path = 'Outputs/combined_data.csv'\n",
    "data_df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert 'value' column to float, handling invalid entries as NaN\n",
    "data_df['value'] = pd.to_numeric(data_df['value'], errors='coerce')\n",
    "\n",
    "# Initialize empty lists to store the data\n",
    "dates = []\n",
    "stations = []\n",
    "tmax_values = []\n",
    "tmin_values = []\n",
    "tavg_values = []\n",
    "prcp_values = []\n",
    "\n",
    "# Iterate through the rows of the original DataFrame\n",
    "for index, row in data_df.iterrows():\n",
    "    date = row['date']\n",
    "    station = row['station']\n",
    "    datatype = row['datatype']\n",
    "    value = row['value']\n",
    "    \n",
    "    if datatype == 'TMAX':\n",
    "        tmax_values.append(value)\n",
    "        tmin_values.append(None)\n",
    "        tavg_values.append(None)\n",
    "        prcp_values.append(None)\n",
    "    elif datatype == 'TMIN':\n",
    "        tmax_values.append(None)\n",
    "        tmin_values.append(value)\n",
    "        tavg_values.append(None)\n",
    "        prcp_values.append(None)\n",
    "    elif datatype == 'TAVG':\n",
    "        tmax_values.append(None)\n",
    "        tmin_values.append(None)\n",
    "        tavg_values.append(value)\n",
    "        prcp_values.append(None)\n",
    "    elif datatype == 'PRCP':\n",
    "        tmax_values.append(None)\n",
    "        tmin_values.append(None)\n",
    "        tavg_values.append(None)\n",
    "        prcp_values.append(value)\n",
    "    \n",
    "    dates.append(date)\n",
    "    stations.append(station)\n",
    "\n",
    "# Create a new DataFrame\n",
    "new_data = {\n",
    "    'date': dates,\n",
    "    'station': stations,\n",
    "    'TMAX': tmax_values,\n",
    "    'TMIN': tmin_values,\n",
    "    'TAVG': tavg_values,\n",
    "    'PRCP': prcp_values\n",
    "}\n",
    "\n",
    "new_df = pd.DataFrame(new_data)\n",
    "\n",
    "# Convert all columns except 'date' and 'station' to float\n",
    "float_columns = new_df.columns.difference(['date', 'station'])\n",
    "new_df[float_columns] = new_df[float_columns].astype(float)\n",
    "\n",
    "# Group by date and station and keep non-null values\n",
    "grouped_df = new_df.groupby(['date', 'station']).first().reset_index()\n",
    "\n",
    "grouped_df = grouped_df.dropna(subset=['TMAX', 'TMIN', 'TAVG','PRCP'])\n",
    "\n",
    "# Write the grouped DataFrame to a CSV file\n",
    "csv_filename = 'Outputs/grouped_df.csv'\n",
    "grouped_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "# Print the first few rows of the grouped DataFrame\n",
    "grouped_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TMAX</th>\n",
       "      <th>TMIN</th>\n",
       "      <th>TAVG</th>\n",
       "      <th>PRCP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>82189.000000</td>\n",
       "      <td>82189.000000</td>\n",
       "      <td>82189.000000</td>\n",
       "      <td>82189.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>18.255401</td>\n",
       "      <td>5.625559</td>\n",
       "      <td>11.940702</td>\n",
       "      <td>84.738435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.941046</td>\n",
       "      <td>10.380864</td>\n",
       "      <td>10.537050</td>\n",
       "      <td>73.186178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-17.210000</td>\n",
       "      <td>-28.710000</td>\n",
       "      <td>-22.790000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.690000</td>\n",
       "      <td>-2.410000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>19.360000</td>\n",
       "      <td>5.770000</td>\n",
       "      <td>12.630000</td>\n",
       "      <td>69.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>27.670000</td>\n",
       "      <td>14.410000</td>\n",
       "      <td>21.040000</td>\n",
       "      <td>122.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>49.190000</td>\n",
       "      <td>35.390000</td>\n",
       "      <td>42.290000</td>\n",
       "      <td>950.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               TMAX          TMIN          TAVG          PRCP\n",
       "count  82189.000000  82189.000000  82189.000000  82189.000000\n",
       "mean      18.255401      5.625559     11.940702     84.738435\n",
       "std       10.941046     10.380864     10.537050     73.186178\n",
       "min      -17.210000    -28.710000    -22.790000      0.000000\n",
       "25%        9.690000     -2.410000      3.600000     28.000000\n",
       "50%       19.360000      5.770000     12.630000     69.200000\n",
       "75%       27.670000     14.410000     21.040000    122.100000\n",
       "max       49.190000     35.390000     42.290000    950.100000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = grouped_df.dropna(subset=['TMAX', 'TMIN', 'TAVG','PRCP'])\n",
    "\n",
    "# Write the grouped DataFrame to a CSV file\n",
    "csv_filename = 'Outputs/grouped_df_detailed.csv'\n",
    "grouped_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "grouped_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder containing the CSV files\n",
    "folder = 'Outputs/US_data'\n",
    "\n",
    "# List of CSV filenames\n",
    "filenames = ['ca_station_temps_2018_2019_1.csv','ca_station_temps_2018_2019.csv']\n",
    "\n",
    "# Initialize an empty DataFrame to store the combined data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Loop through the filenames and read each CSV into a DataFrame\n",
    "for filename in filenames:\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "# Remove duplicates from the combined DataFrame\n",
    "combined_df = combined_df.drop_duplicates()\n",
    "\n",
    "# Save the combined and deduplicated DataFrame to a new CSV file\n",
    "output_filename = os.path.join('Outputs/combined_temps.csv')\n",
    "combined_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"Combined CSVs and removed duplicates. Saved as '{output_filename}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from your CSV or Excel file\n",
    "file_path = 'Outputs/combined_temps.csv'\n",
    "data_df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert 'value' column to float, handling invalid entries as NaN\n",
    "data_df['value'] = pd.to_numeric(data_df['value'], errors='coerce')\n",
    "\n",
    "# Initialize empty lists to store the data\n",
    "dates = []\n",
    "stations = []\n",
    "tmax_values = []\n",
    "tmin_values = []\n",
    "tavg_values = []\n",
    "prcp_values = []\n",
    "\n",
    "# Iterate through the rows of the original DataFrame\n",
    "for index, row in data_df.iterrows():\n",
    "    date = row['date']\n",
    "    station = row['station']\n",
    "    datatype = row['datatype']\n",
    "    value = row['value']\n",
    "    \n",
    "    if datatype == 'TMAX':\n",
    "        tmax_values.append(value)\n",
    "        tmin_values.append(None)\n",
    "        tavg_values.append(None)\n",
    "        prcp_values.append(None)\n",
    "    elif datatype == 'TMIN':\n",
    "        tmax_values.append(None)\n",
    "        tmin_values.append(value)\n",
    "        tavg_values.append(None)\n",
    "        prcp_values.append(None)\n",
    "    elif datatype == 'TAVG':\n",
    "        tmax_values.append(None)\n",
    "        tmin_values.append(None)\n",
    "        tavg_values.append(value)\n",
    "        prcp_values.append(None)\n",
    "    elif datatype == 'PRCP':\n",
    "        tmax_values.append(None)\n",
    "        tmin_values.append(None)\n",
    "        tavg_values.append(None)\n",
    "        prcp_values.append(value)\n",
    "    \n",
    "    dates.append(date)\n",
    "    stations.append(station)\n",
    "\n",
    "# Create a new DataFrame\n",
    "new_data = {\n",
    "    'date': dates,\n",
    "    'station': stations,\n",
    "    'TMAX': tmax_values,\n",
    "    'TMIN': tmin_values,\n",
    "    'TAVG': tavg_values,\n",
    "    'PRCP': prcp_values\n",
    "}\n",
    "\n",
    "new_df = pd.DataFrame(new_data)\n",
    "\n",
    "# Convert all columns except 'date' and 'station' to float\n",
    "float_columns = new_df.columns.difference(['date', 'station'])\n",
    "new_df[float_columns] = new_df[float_columns].astype(float)\n",
    "\n",
    "# Group by date and station and keep non-null values\n",
    "grouped_df = new_df.groupby(['date', 'station']).first().reset_index()\n",
    "\n",
    "# Drop rows with NaN values in 'TMAX', 'TMIN', and 'TAVG' columns\n",
    "subset_columns = ['TMAX', 'TMIN', 'TAVG']\n",
    "grouped_df = grouped_df.dropna(subset=subset_columns, how='all')\n",
    "\n",
    "# Write the grouped DataFrame to a CSV file\n",
    "csv_filename = 'Outputs/grouped_df.csv'\n",
    "grouped_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "# Print the first few rows of the grouped DataFrame\n",
    "grouped_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>elevation</th>\n",
       "      <th>date</th>\n",
       "      <th>maxdate</th>\n",
       "      <th>mindate</th>\n",
       "      <th>TAVG</th>\n",
       "      <th>TMAX</th>\n",
       "      <th>TMIN</th>\n",
       "      <th>PRCP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GHCND:AQW00061705</td>\n",
       "      <td>PAGO PAGO WEATHER SERVICE OFFICE AIRPORT, US</td>\n",
       "      <td>-14.33056</td>\n",
       "      <td>-170.71361</td>\n",
       "      <td>3.7</td>\n",
       "      <td>2018-01-01T00:00:00</td>\n",
       "      <td>2023-07-31</td>\n",
       "      <td>1945-08-01</td>\n",
       "      <td>27.85</td>\n",
       "      <td>30.58</td>\n",
       "      <td>25.12</td>\n",
       "      <td>390.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GHCND:CA001018611</td>\n",
       "      <td>VICTORIA GONZALES CS, WA US</td>\n",
       "      <td>48.03330</td>\n",
       "      <td>-123.33330</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2018-01-01T00:00:00</td>\n",
       "      <td>2023-07-31</td>\n",
       "      <td>1973-01-01</td>\n",
       "      <td>6.30</td>\n",
       "      <td>7.98</td>\n",
       "      <td>4.63</td>\n",
       "      <td>117.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GHCND:CA001135126</td>\n",
       "      <td>MIDWAY, WA US</td>\n",
       "      <td>49.00000</td>\n",
       "      <td>-118.76670</td>\n",
       "      <td>580.0</td>\n",
       "      <td>2018-01-01T00:00:00</td>\n",
       "      <td>2023-06-11</td>\n",
       "      <td>1987-06-01</td>\n",
       "      <td>-2.17</td>\n",
       "      <td>1.10</td>\n",
       "      <td>-5.44</td>\n",
       "      <td>67.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GHCND:CA005020881</td>\n",
       "      <td>EMERSON AUTO, ND US</td>\n",
       "      <td>49.00000</td>\n",
       "      <td>-97.23330</td>\n",
       "      <td>242.0</td>\n",
       "      <td>2018-01-01T00:00:00</td>\n",
       "      <td>2023-07-31</td>\n",
       "      <td>2009-07-01</td>\n",
       "      <td>-13.40</td>\n",
       "      <td>-8.93</td>\n",
       "      <td>-17.87</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GHCND:CA006020559</td>\n",
       "      <td>BARWICK, MN US</td>\n",
       "      <td>48.63330</td>\n",
       "      <td>-93.96670</td>\n",
       "      <td>335.0</td>\n",
       "      <td>2018-01-01T00:00:00</td>\n",
       "      <td>2023-07-31</td>\n",
       "      <td>1978-12-01</td>\n",
       "      <td>-14.14</td>\n",
       "      <td>-8.50</td>\n",
       "      <td>-19.77</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             station                                          name  latitude  \\\n",
       "0  GHCND:AQW00061705  PAGO PAGO WEATHER SERVICE OFFICE AIRPORT, US -14.33056   \n",
       "1  GHCND:CA001018611                   VICTORIA GONZALES CS, WA US  48.03330   \n",
       "2  GHCND:CA001135126                                 MIDWAY, WA US  49.00000   \n",
       "3  GHCND:CA005020881                           EMERSON AUTO, ND US  49.00000   \n",
       "4  GHCND:CA006020559                                BARWICK, MN US  48.63330   \n",
       "\n",
       "   longitude  elevation                 date     maxdate     mindate   TAVG  \\\n",
       "0 -170.71361        3.7  2018-01-01T00:00:00  2023-07-31  1945-08-01  27.85   \n",
       "1 -123.33330       70.0  2018-01-01T00:00:00  2023-07-31  1973-01-01   6.30   \n",
       "2 -118.76670      580.0  2018-01-01T00:00:00  2023-06-11  1987-06-01  -2.17   \n",
       "3  -97.23330      242.0  2018-01-01T00:00:00  2023-07-31  2009-07-01 -13.40   \n",
       "4  -93.96670      335.0  2018-01-01T00:00:00  2023-07-31  1978-12-01 -14.14   \n",
       "\n",
       "    TMAX   TMIN   PRCP  \n",
       "0  30.58  25.12  390.3  \n",
       "1   7.98   4.63  117.6  \n",
       "2   1.10  -5.44   67.6  \n",
       "3  -8.93 -17.87    3.6  \n",
       "4  -8.50 -19.77   21.6  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the stations.csv file into stations_df\n",
    "stations_df = pd.read_csv('Outputs/full_station_list.csv')\n",
    "grouped_df= pd.read_csv('Outputs/grouped_df.csv')\n",
    "\n",
    "# Merge the two DataFrames based on 'station' using a left join\n",
    "grouped_df_detailed = pd.merge(grouped_df, stations_df, \n",
    "                               left_on='station', right_on='id', how='left')\n",
    "\n",
    "# Drop the redundant columns (from stations_df)\n",
    "grouped_df_detailed.drop(columns=['id'], inplace=True)\n",
    "\n",
    "grouped_df_detailed=grouped_df_detailed[['station','name','latitude','longitude','elevation','date','maxdate','mindate','TAVG','TMAX','TMIN','PRCP']]\n",
    "\n",
    "# Write the grouped DataFrame to a CSV file\n",
    "csv_filename = 'Outputs/grouped_df_detailed.csv'\n",
    "grouped_df_detailed.to_csv(csv_filename, index=False)\n",
    "\n",
    "grouped_df_detailed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wildfire=pd.read_csv('Resources/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['OBJECTID', 'Shape', 'FOD_ID', 'FPA_ID', 'SOURCE_SYSTEM_TYPE',\n",
       "       'SOURCE_SYSTEM', 'NWCG_REPORTING_AGENCY', 'NWCG_REPORTING_UNIT_ID',\n",
       "       'NWCG_REPORTING_UNIT_NAME', 'SOURCE_REPORTING_UNIT',\n",
       "       'SOURCE_REPORTING_UNIT_NAME', 'LOCAL_FIRE_REPORT_ID',\n",
       "       'LOCAL_INCIDENT_ID', 'FIRE_CODE', 'FIRE_NAME',\n",
       "       'ICS_209_PLUS_INCIDENT_JOIN_ID', 'ICS_209_PLUS_COMPLEX_JOIN_ID',\n",
       "       'MTBS_ID', 'MTBS_FIRE_NAME', 'COMPLEX_NAME', 'FIRE_YEAR',\n",
       "       'DISCOVERY_DATE', 'DISCOVERY_DOY', 'DISCOVERY_TIME',\n",
       "       'NWCG_CAUSE_CLASSIFICATION', 'NWCG_GENERAL_CAUSE',\n",
       "       'NWCG_CAUSE_AGE_CATEGORY', 'CONT_DATE', 'CONT_DOY', 'CONT_TIME',\n",
       "       'FIRE_SIZE', 'FIRE_SIZE_CLASS', 'LATITUDE', 'LONGITUDE', 'OWNER_DESCR',\n",
       "       'STATE', 'COUNTY', 'FIPS_CODE', 'FIPS_NAME'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wildfire.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>FIRE_SIZE</th>\n",
       "      <th>NWCG_REPORTING_UNIT_NAME</th>\n",
       "      <th>FIRE_SIZE_CLASS</th>\n",
       "      <th>FIRE_YEAR</th>\n",
       "      <th>FPA_ID</th>\n",
       "      <th>FIRE_CODE</th>\n",
       "      <th>NWCG_CAUSE_CLASSIFICATION</th>\n",
       "      <th>NWCG_GENERAL_CAUSE</th>\n",
       "      <th>FIRE_NAME</th>\n",
       "      <th>DISCOVERY_DATE</th>\n",
       "      <th>CONT_DATE</th>\n",
       "      <th>DISCOVERY_TIME</th>\n",
       "      <th>CONT_TIME</th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>FIPS_CODE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2045714</th>\n",
       "      <td>46.275833</td>\n",
       "      <td>-114.379167</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Bitterroot National Forest</td>\n",
       "      <td>A</td>\n",
       "      <td>2018</td>\n",
       "      <td>FS-6911076</td>\n",
       "      <td>EKS4</td>\n",
       "      <td>Natural</td>\n",
       "      <td>Natural</td>\n",
       "      <td>BLODGETT</td>\n",
       "      <td>8/22/2018</td>\n",
       "      <td>8/22/2018</td>\n",
       "      <td>1625.0</td>\n",
       "      <td>1740.0</td>\n",
       "      <td>MT</td>\n",
       "      <td>081</td>\n",
       "      <td>30081.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2045715</th>\n",
       "      <td>46.404167</td>\n",
       "      <td>-113.921944</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Bitterroot National Forest</td>\n",
       "      <td>A</td>\n",
       "      <td>2018</td>\n",
       "      <td>FS-6908885</td>\n",
       "      <td>L1RX</td>\n",
       "      <td>Human</td>\n",
       "      <td>Equipment and vehicle use</td>\n",
       "      <td>CORLEY GULCH</td>\n",
       "      <td>7/26/2018</td>\n",
       "      <td>7/28/2018</td>\n",
       "      <td>1225.0</td>\n",
       "      <td>1653.0</td>\n",
       "      <td>MT</td>\n",
       "      <td>081</td>\n",
       "      <td>30081.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2045716</th>\n",
       "      <td>46.245833</td>\n",
       "      <td>-114.308889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Bitterroot National Forest</td>\n",
       "      <td>B</td>\n",
       "      <td>2018</td>\n",
       "      <td>FS-6898061</td>\n",
       "      <td>L49X</td>\n",
       "      <td>Human</td>\n",
       "      <td>Recreation and ceremony</td>\n",
       "      <td>CANYON CREEK</td>\n",
       "      <td>9/21/2018</td>\n",
       "      <td>9/23/2018</td>\n",
       "      <td>1305.0</td>\n",
       "      <td>1241.0</td>\n",
       "      <td>MT</td>\n",
       "      <td>081</td>\n",
       "      <td>30081.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2045717</th>\n",
       "      <td>45.784722</td>\n",
       "      <td>-114.033056</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Bitterroot National Forest</td>\n",
       "      <td>A</td>\n",
       "      <td>2018</td>\n",
       "      <td>FS-6890683</td>\n",
       "      <td>EKS4</td>\n",
       "      <td>Natural</td>\n",
       "      <td>Natural</td>\n",
       "      <td>MAYNARD CREEK</td>\n",
       "      <td>8/17/2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1723.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MT</td>\n",
       "      <td>081</td>\n",
       "      <td>30081.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2045718</th>\n",
       "      <td>45.986944</td>\n",
       "      <td>-113.807222</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Bitterroot National Forest</td>\n",
       "      <td>A</td>\n",
       "      <td>2018</td>\n",
       "      <td>FS-6888073</td>\n",
       "      <td>EKS4</td>\n",
       "      <td>Natural</td>\n",
       "      <td>Natural</td>\n",
       "      <td>BLUE</td>\n",
       "      <td>8/12/2018</td>\n",
       "      <td>8/12/2018</td>\n",
       "      <td>1031.0</td>\n",
       "      <td>1334.0</td>\n",
       "      <td>MT</td>\n",
       "      <td>081</td>\n",
       "      <td>30081.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          LATITUDE   LONGITUDE  FIRE_SIZE    NWCG_REPORTING_UNIT_NAME  \\\n",
       "2045714  46.275833 -114.379167        0.1  Bitterroot National Forest   \n",
       "2045715  46.404167 -113.921944        0.1  Bitterroot National Forest   \n",
       "2045716  46.245833 -114.308889        1.0  Bitterroot National Forest   \n",
       "2045717  45.784722 -114.033056        0.1  Bitterroot National Forest   \n",
       "2045718  45.986944 -113.807222        0.1  Bitterroot National Forest   \n",
       "\n",
       "        FIRE_SIZE_CLASS  FIRE_YEAR      FPA_ID FIRE_CODE  \\\n",
       "2045714               A       2018  FS-6911076      EKS4   \n",
       "2045715               A       2018  FS-6908885      L1RX   \n",
       "2045716               B       2018  FS-6898061      L49X   \n",
       "2045717               A       2018  FS-6890683      EKS4   \n",
       "2045718               A       2018  FS-6888073      EKS4   \n",
       "\n",
       "        NWCG_CAUSE_CLASSIFICATION         NWCG_GENERAL_CAUSE      FIRE_NAME  \\\n",
       "2045714                   Natural                    Natural       BLODGETT   \n",
       "2045715                     Human  Equipment and vehicle use   CORLEY GULCH   \n",
       "2045716                     Human    Recreation and ceremony   CANYON CREEK   \n",
       "2045717                   Natural                    Natural  MAYNARD CREEK   \n",
       "2045718                   Natural                    Natural           BLUE   \n",
       "\n",
       "        DISCOVERY_DATE  CONT_DATE  DISCOVERY_TIME  CONT_TIME STATE COUNTY  \\\n",
       "2045714      8/22/2018  8/22/2018          1625.0     1740.0    MT    081   \n",
       "2045715      7/26/2018  7/28/2018          1225.0     1653.0    MT    081   \n",
       "2045716      9/21/2018  9/23/2018          1305.0     1241.0    MT    081   \n",
       "2045717      8/17/2018        NaN          1723.0        NaN    MT    081   \n",
       "2045718      8/12/2018  8/12/2018          1031.0     1334.0    MT    081   \n",
       "\n",
       "         FIPS_CODE  \n",
       "2045714    30081.0  \n",
       "2045715    30081.0  \n",
       "2045716    30081.0  \n",
       "2045717    30081.0  \n",
       "2045718    30081.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wildfires=wildfire[['LATITUDE', 'LONGITUDE', 'FIRE_SIZE','NWCG_REPORTING_UNIT_NAME', 'FIRE_SIZE_CLASS', 'FIRE_YEAR',\n",
    "       'FPA_ID', 'FIRE_CODE', 'NWCG_CAUSE_CLASSIFICATION',\n",
    "       'NWCG_GENERAL_CAUSE', 'FIRE_NAME', 'DISCOVERY_DATE', 'CONT_DATE',\n",
    "       'DISCOVERY_TIME', 'CONT_TIME', 'STATE', 'COUNTY', 'FIPS_CODE'\n",
    "       ]]\n",
    "wildfires=wildfires[\n",
    "    (wildfires['FIRE_YEAR'].isin([2018,2019]))\n",
    "]\n",
    "wildfires.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144417, 18)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wildfires.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>FIPS_CODE</th>\n",
       "      <th>FIRE_SIZE</th>\n",
       "      <th>FIRE_SIZE_CLASS</th>\n",
       "      <th>NWCG_CAUSE_CLASSIFICATION</th>\n",
       "      <th>NWCG_GENERAL_CAUSE</th>\n",
       "      <th>FIRE_NAME</th>\n",
       "      <th>DISCOVERY_DATE</th>\n",
       "      <th>...</th>\n",
       "      <th>station</th>\n",
       "      <th>name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>elevation</th>\n",
       "      <th>date</th>\n",
       "      <th>TAVG</th>\n",
       "      <th>TMAX</th>\n",
       "      <th>TMIN</th>\n",
       "      <th>PRCP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46.275833</td>\n",
       "      <td>-114.379167</td>\n",
       "      <td>081</td>\n",
       "      <td>30081.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>A</td>\n",
       "      <td>Natural</td>\n",
       "      <td>Natural</td>\n",
       "      <td>BLODGETT</td>\n",
       "      <td>2018-08-22</td>\n",
       "      <td>...</td>\n",
       "      <td>GHCND:USC00243885</td>\n",
       "      <td>HAMILTON, MT US</td>\n",
       "      <td>46.24622</td>\n",
       "      <td>-114.16794</td>\n",
       "      <td>1092.7</td>\n",
       "      <td>2018-09-01</td>\n",
       "      <td>12.89</td>\n",
       "      <td>21.95</td>\n",
       "      <td>3.82</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46.404167</td>\n",
       "      <td>-113.921944</td>\n",
       "      <td>081</td>\n",
       "      <td>30081.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>A</td>\n",
       "      <td>Human</td>\n",
       "      <td>Equipment and vehicle use</td>\n",
       "      <td>CORLEY GULCH</td>\n",
       "      <td>2018-07-26</td>\n",
       "      <td>...</td>\n",
       "      <td>GHCND:USC00247894</td>\n",
       "      <td>STEVENSVILLE, MT US</td>\n",
       "      <td>46.5137</td>\n",
       "      <td>-114.091</td>\n",
       "      <td>1028.7</td>\n",
       "      <td>2018-08-01</td>\n",
       "      <td>18.29</td>\n",
       "      <td>28.05</td>\n",
       "      <td>8.53</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46.245833</td>\n",
       "      <td>-114.308889</td>\n",
       "      <td>081</td>\n",
       "      <td>30081.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>B</td>\n",
       "      <td>Human</td>\n",
       "      <td>Recreation and ceremony</td>\n",
       "      <td>CANYON CREEK</td>\n",
       "      <td>2018-09-21</td>\n",
       "      <td>...</td>\n",
       "      <td>GHCND:USC00243885</td>\n",
       "      <td>HAMILTON, MT US</td>\n",
       "      <td>46.24622</td>\n",
       "      <td>-114.16794</td>\n",
       "      <td>1092.7</td>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>6.96</td>\n",
       "      <td>13.28</td>\n",
       "      <td>0.65</td>\n",
       "      <td>33.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45.784722</td>\n",
       "      <td>-114.033056</td>\n",
       "      <td>081</td>\n",
       "      <td>30081.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>A</td>\n",
       "      <td>Natural</td>\n",
       "      <td>Natural</td>\n",
       "      <td>MAYNARD CREEK</td>\n",
       "      <td>2018-08-17</td>\n",
       "      <td>...</td>\n",
       "      <td>GHCND:USC00242221</td>\n",
       "      <td>DARBY, MT US</td>\n",
       "      <td>46.0263</td>\n",
       "      <td>-114.1763</td>\n",
       "      <td>1182.6</td>\n",
       "      <td>2018-09-01</td>\n",
       "      <td>11.77</td>\n",
       "      <td>20.79</td>\n",
       "      <td>2.74</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45.986944</td>\n",
       "      <td>-113.807222</td>\n",
       "      <td>081</td>\n",
       "      <td>30081.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>A</td>\n",
       "      <td>Natural</td>\n",
       "      <td>Natural</td>\n",
       "      <td>BLUE</td>\n",
       "      <td>2018-08-12</td>\n",
       "      <td>...</td>\n",
       "      <td>GHCND:USC00247967</td>\n",
       "      <td>SULA 14 NE, MT US</td>\n",
       "      <td>45.911</td>\n",
       "      <td>-113.7394</td>\n",
       "      <td>1571.2</td>\n",
       "      <td>2018-08-01</td>\n",
       "      <td>15.1</td>\n",
       "      <td>26.09</td>\n",
       "      <td>4.12</td>\n",
       "      <td>19.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>46.023056</td>\n",
       "      <td>-113.799722</td>\n",
       "      <td>081</td>\n",
       "      <td>30081.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>A</td>\n",
       "      <td>Natural</td>\n",
       "      <td>Natural</td>\n",
       "      <td>POLLYWOG</td>\n",
       "      <td>2018-08-11</td>\n",
       "      <td>...</td>\n",
       "      <td>GHCND:USC00247967</td>\n",
       "      <td>SULA 14 NE, MT US</td>\n",
       "      <td>45.911</td>\n",
       "      <td>-113.7394</td>\n",
       "      <td>1571.2</td>\n",
       "      <td>2018-08-01</td>\n",
       "      <td>15.1</td>\n",
       "      <td>26.09</td>\n",
       "      <td>4.12</td>\n",
       "      <td>19.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>45.913611</td>\n",
       "      <td>-114.6675</td>\n",
       "      <td>049</td>\n",
       "      <td>16049.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>A</td>\n",
       "      <td>Natural</td>\n",
       "      <td>Natural</td>\n",
       "      <td>CEDAR</td>\n",
       "      <td>2018-07-25</td>\n",
       "      <td>...</td>\n",
       "      <td>GHCND:USC00108246</td>\n",
       "      <td>SELWAY LODGE, ID US</td>\n",
       "      <td>46.0081</td>\n",
       "      <td>-114.8442</td>\n",
       "      <td>786.4</td>\n",
       "      <td>2018-08-01</td>\n",
       "      <td>19.38</td>\n",
       "      <td>31.89</td>\n",
       "      <td>6.88</td>\n",
       "      <td>21.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>46.121111</td>\n",
       "      <td>-114.239167</td>\n",
       "      <td>081</td>\n",
       "      <td>30081.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>A</td>\n",
       "      <td>Natural</td>\n",
       "      <td>Natural</td>\n",
       "      <td>DOUBLE STRIKE</td>\n",
       "      <td>2018-06-08</td>\n",
       "      <td>...</td>\n",
       "      <td>GHCND:USC00242221</td>\n",
       "      <td>DARBY, MT US</td>\n",
       "      <td>46.0263</td>\n",
       "      <td>-114.1763</td>\n",
       "      <td>1182.6</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.15</td>\n",
       "      <td>8.09</td>\n",
       "      <td>102.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>45.868333</td>\n",
       "      <td>-113.804167</td>\n",
       "      <td>081</td>\n",
       "      <td>30081.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>A</td>\n",
       "      <td>Natural</td>\n",
       "      <td>Natural</td>\n",
       "      <td>MEADOW</td>\n",
       "      <td>2018-08-17</td>\n",
       "      <td>...</td>\n",
       "      <td>GHCND:USC00247967</td>\n",
       "      <td>SULA 14 NE, MT US</td>\n",
       "      <td>45.911</td>\n",
       "      <td>-113.7394</td>\n",
       "      <td>1571.2</td>\n",
       "      <td>2018-09-01</td>\n",
       "      <td>9.69</td>\n",
       "      <td>20.79</td>\n",
       "      <td>-1.42</td>\n",
       "      <td>11.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>45.914444</td>\n",
       "      <td>-114.635278</td>\n",
       "      <td>049</td>\n",
       "      <td>16049.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>A</td>\n",
       "      <td>Natural</td>\n",
       "      <td>Natural</td>\n",
       "      <td>MT GEORGE 2</td>\n",
       "      <td>2018-07-24</td>\n",
       "      <td>...</td>\n",
       "      <td>GHCND:USC00108246</td>\n",
       "      <td>SELWAY LODGE, ID US</td>\n",
       "      <td>46.0081</td>\n",
       "      <td>-114.8442</td>\n",
       "      <td>786.4</td>\n",
       "      <td>2018-08-01</td>\n",
       "      <td>19.38</td>\n",
       "      <td>31.89</td>\n",
       "      <td>6.88</td>\n",
       "      <td>21.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    LATITUDE   LONGITUDE COUNTY FIPS_CODE FIRE_SIZE FIRE_SIZE_CLASS  \\\n",
       "0  46.275833 -114.379167    081   30081.0       0.1               A   \n",
       "1  46.404167 -113.921944    081   30081.0       0.1               A   \n",
       "2  46.245833 -114.308889    081   30081.0       1.0               B   \n",
       "3  45.784722 -114.033056    081   30081.0       0.1               A   \n",
       "4  45.986944 -113.807222    081   30081.0       0.1               A   \n",
       "5  46.023056 -113.799722    081   30081.0       0.1               A   \n",
       "6  45.913611   -114.6675    049   16049.0       0.1               A   \n",
       "7  46.121111 -114.239167    081   30081.0       0.1               A   \n",
       "8  45.868333 -113.804167    081   30081.0      0.25               A   \n",
       "9  45.914444 -114.635278    049   16049.0       0.1               A   \n",
       "\n",
       "  NWCG_CAUSE_CLASSIFICATION         NWCG_GENERAL_CAUSE      FIRE_NAME  \\\n",
       "0                   Natural                    Natural       BLODGETT   \n",
       "1                     Human  Equipment and vehicle use   CORLEY GULCH   \n",
       "2                     Human    Recreation and ceremony   CANYON CREEK   \n",
       "3                   Natural                    Natural  MAYNARD CREEK   \n",
       "4                   Natural                    Natural           BLUE   \n",
       "5                   Natural                    Natural       POLLYWOG   \n",
       "6                   Natural                    Natural          CEDAR   \n",
       "7                   Natural                    Natural  DOUBLE STRIKE   \n",
       "8                   Natural                    Natural         MEADOW   \n",
       "9                   Natural                    Natural    MT GEORGE 2   \n",
       "\n",
       "  DISCOVERY_DATE  ...            station                 name  latitude  \\\n",
       "0     2018-08-22  ...  GHCND:USC00243885      HAMILTON, MT US  46.24622   \n",
       "1     2018-07-26  ...  GHCND:USC00247894  STEVENSVILLE, MT US   46.5137   \n",
       "2     2018-09-21  ...  GHCND:USC00243885      HAMILTON, MT US  46.24622   \n",
       "3     2018-08-17  ...  GHCND:USC00242221         DARBY, MT US   46.0263   \n",
       "4     2018-08-12  ...  GHCND:USC00247967    SULA 14 NE, MT US    45.911   \n",
       "5     2018-08-11  ...  GHCND:USC00247967    SULA 14 NE, MT US    45.911   \n",
       "6     2018-07-25  ...  GHCND:USC00108246  SELWAY LODGE, ID US   46.0081   \n",
       "7     2018-06-08  ...  GHCND:USC00242221         DARBY, MT US   46.0263   \n",
       "8     2018-08-17  ...  GHCND:USC00247967    SULA 14 NE, MT US    45.911   \n",
       "9     2018-07-24  ...  GHCND:USC00108246  SELWAY LODGE, ID US   46.0081   \n",
       "\n",
       "   longitude elevation       date   TAVG   TMAX  TMIN   PRCP  \n",
       "0 -114.16794    1092.7 2018-09-01  12.89  21.95  3.82    3.5  \n",
       "1   -114.091    1028.7 2018-08-01  18.29  28.05  8.53    8.6  \n",
       "2 -114.16794    1092.7 2018-10-01   6.96  13.28  0.65   33.7  \n",
       "3  -114.1763    1182.6 2018-09-01  11.77  20.79  2.74    9.1  \n",
       "4  -113.7394    1571.2 2018-08-01   15.1  26.09  4.12   19.4  \n",
       "5  -113.7394    1571.2 2018-08-01   15.1  26.09  4.12   19.4  \n",
       "6  -114.8442     786.4 2018-08-01  19.38  31.89  6.88   21.1  \n",
       "7  -114.1763    1182.6 2018-06-01  14.62  21.15  8.09  102.7  \n",
       "8  -113.7394    1571.2 2018-09-01   9.69  20.79 -1.42   11.2  \n",
       "9  -114.8442     786.4 2018-08-01  19.38  31.89  6.88   21.1  \n",
       "\n",
       "[10 rows x 24 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Takes 4-10 minutes to run 1 year, 25-30  minutes for 2 years\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "wildfires['DISCOVERY_DATE'] = pd.to_datetime(wildfires['DISCOVERY_DATE'])\n",
    "grouped_df_detailed['date'] = pd.to_datetime(grouped_df_detailed['date'])\n",
    "\n",
    "def find_nearest_match(row, df, date_col, lat_col, lon_col):\n",
    "    date_diff = abs((df[date_col] - row['DISCOVERY_DATE']).dt.total_seconds())\n",
    "    lat_diff = abs(df[lat_col] - row['LATITUDE'])\n",
    "    lon_diff = abs(df[lon_col] - row['LONGITUDE'])\n",
    "    total_diff = date_diff + lat_diff + lon_diff\n",
    "    nearest_idx = total_diff.idxmin()\n",
    "    return df.loc[nearest_idx]\n",
    "\n",
    "# Create an empty list to hold the merged rows\n",
    "merged_rows = []\n",
    "\n",
    "# Iterate over each row in the filtered wildfires DataFrame\n",
    "for idx, row in wildfires.iterrows():\n",
    "    nearest_row = find_nearest_match(row, grouped_df_detailed, 'date', 'latitude', 'longitude')\n",
    "    merged_row = pd.concat([row, nearest_row])\n",
    "    merged_rows.append(merged_row)\n",
    "\n",
    "# Concatenate the list of merged rows into a DataFrame\n",
    "merged_results = pd.concat(merged_rows, axis=1).T\n",
    "\n",
    "merged_results=merged_results[['LATITUDE', 'LONGITUDE','COUNTY', 'FIPS_CODE','FIRE_SIZE', 'FIRE_SIZE_CLASS',\n",
    "       'NWCG_CAUSE_CLASSIFICATION', 'NWCG_GENERAL_CAUSE', 'FIRE_NAME',\n",
    "       'DISCOVERY_DATE', 'CONT_DATE', 'DISCOVERY_TIME', 'CONT_TIME', 'STATE',\n",
    "       'station', 'name', 'latitude', 'longitude', 'elevation', 'date', 'TAVG',\n",
    "       'TMAX', 'TMIN','PRCP']]\n",
    "\n",
    "csv_filename = 'Outputs/merged_results.csv'\n",
    "merged_results.to_csv(csv_filename, index=False)\n",
    "\n",
    "merged_results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name\n",
       "LITCHFIELD PARK, AZ US           1692\n",
       "VICTORIA GONZALES CS, WA US      1480\n",
       "EAST MESA, AZ US                 1258\n",
       "TEMPE ASU, AZ US                 1071\n",
       "TOHONO CHUL, AZ US               1009\n",
       "                                 ... \n",
       "HACHITA 1 W, NM US                  1\n",
       "VAN HORN, TX US                     1\n",
       "FITTSTOWN 6 SW MESONET, OK US       1\n",
       "HEALDTON 3 E, OK US                 1\n",
       "SAINT FRANCIS, KS US                1\n",
       "Name: count, Length: 3526, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_results['name'].value_counts()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIRE_NAME</th>\n",
       "      <th>STATE</th>\n",
       "      <th>FIPS_CODE</th>\n",
       "      <th>FIRE_LATITUDE</th>\n",
       "      <th>FIRE_LONGITUDE</th>\n",
       "      <th>FIRE_DATE</th>\n",
       "      <th>CONTAIN_DATE</th>\n",
       "      <th>CLOSEST_STATION</th>\n",
       "      <th>STATION_LAT</th>\n",
       "      <th>STATION_LON</th>\n",
       "      <th>READINGS_DATE</th>\n",
       "      <th>CAUSE_CLASSIFICATION</th>\n",
       "      <th>FIRE_SIZE</th>\n",
       "      <th>FIRE_SIZE_CLASS</th>\n",
       "      <th>ELEVATION</th>\n",
       "      <th>TAVG</th>\n",
       "      <th>TMAX</th>\n",
       "      <th>TMIN</th>\n",
       "      <th>PRCP</th>\n",
       "      <th>DAYS_TO_CONTAIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>144407</th>\n",
       "      <td>BARREN HILL</td>\n",
       "      <td>ID</td>\n",
       "      <td>16049.0</td>\n",
       "      <td>46.236944</td>\n",
       "      <td>-114.982800</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>SELWAY LODGE, ID US</td>\n",
       "      <td>46.00810</td>\n",
       "      <td>-114.84420</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>Natural</td>\n",
       "      <td>1592.0</td>\n",
       "      <td>F</td>\n",
       "      <td>786.4</td>\n",
       "      <td>19.28</td>\n",
       "      <td>31.05</td>\n",
       "      <td>7.51</td>\n",
       "      <td>23.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144408</th>\n",
       "      <td>SAN RAFAEL</td>\n",
       "      <td>AZ</td>\n",
       "      <td>4023.0</td>\n",
       "      <td>31.423040</td>\n",
       "      <td>-110.571000</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>PATAGONIA PATON CENTER, AZ US</td>\n",
       "      <td>31.53923</td>\n",
       "      <td>-110.76028</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>Natural</td>\n",
       "      <td>438.0</td>\n",
       "      <td>E</td>\n",
       "      <td>1232.6</td>\n",
       "      <td>25.41</td>\n",
       "      <td>35.49</td>\n",
       "      <td>15.34</td>\n",
       "      <td>65.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144409</th>\n",
       "      <td>ROCK</td>\n",
       "      <td>CA</td>\n",
       "      <td>6099.0</td>\n",
       "      <td>37.472222</td>\n",
       "      <td>-121.249444</td>\n",
       "      <td>2019-06-25</td>\n",
       "      <td>NaT</td>\n",
       "      <td>TRACY CARBONA, CA US</td>\n",
       "      <td>37.68190</td>\n",
       "      <td>-121.34660</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>Missing data/not specified/undetermined</td>\n",
       "      <td>2422.0</td>\n",
       "      <td>F</td>\n",
       "      <td>41.1</td>\n",
       "      <td>24.63</td>\n",
       "      <td>34.24</td>\n",
       "      <td>15.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144410</th>\n",
       "      <td>CONNEX WF</td>\n",
       "      <td>FL</td>\n",
       "      <td>12091.0</td>\n",
       "      <td>30.523333</td>\n",
       "      <td>-86.781667</td>\n",
       "      <td>2019-10-07</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NICEVILLE, FL US</td>\n",
       "      <td>30.53160</td>\n",
       "      <td>-86.49280</td>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>Human</td>\n",
       "      <td>970.0</td>\n",
       "      <td>E</td>\n",
       "      <td>22.6</td>\n",
       "      <td>21.55</td>\n",
       "      <td>28.44</td>\n",
       "      <td>14.66</td>\n",
       "      <td>146.7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144411</th>\n",
       "      <td>BEAVER POND</td>\n",
       "      <td>MS</td>\n",
       "      <td>28153.0</td>\n",
       "      <td>31.493330</td>\n",
       "      <td>-88.740280</td>\n",
       "      <td>2019-06-13</td>\n",
       "      <td>NaT</td>\n",
       "      <td>WAYNESBORO 2 W, MS US</td>\n",
       "      <td>31.67730</td>\n",
       "      <td>-88.67090</td>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>Human</td>\n",
       "      <td>168.0</td>\n",
       "      <td>D</td>\n",
       "      <td>61.0</td>\n",
       "      <td>26.21</td>\n",
       "      <td>32.35</td>\n",
       "      <td>20.06</td>\n",
       "      <td>91.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144412</th>\n",
       "      <td>BEN HOWARD HOLLOW</td>\n",
       "      <td>KY</td>\n",
       "      <td>21013.0</td>\n",
       "      <td>36.850278</td>\n",
       "      <td>-83.506667</td>\n",
       "      <td>2019-09-21</td>\n",
       "      <td>NaT</td>\n",
       "      <td>HARLAN 3 S, KY US</td>\n",
       "      <td>36.80580</td>\n",
       "      <td>-83.34410</td>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>Human</td>\n",
       "      <td>272.0</td>\n",
       "      <td>D</td>\n",
       "      <td>378.0</td>\n",
       "      <td>15.24</td>\n",
       "      <td>22.17</td>\n",
       "      <td>8.31</td>\n",
       "      <td>104.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144413</th>\n",
       "      <td>2019-3354</td>\n",
       "      <td>KS</td>\n",
       "      <td>20139.0</td>\n",
       "      <td>36.711640</td>\n",
       "      <td>-96.740750</td>\n",
       "      <td>2019-11-25</td>\n",
       "      <td>NaT</td>\n",
       "      <td>RALSTON, OK US</td>\n",
       "      <td>36.50440</td>\n",
       "      <td>-96.74380</td>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>Human</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>F</td>\n",
       "      <td>251.5</td>\n",
       "      <td>5.92</td>\n",
       "      <td>12.90</td>\n",
       "      <td>-1.06</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144414</th>\n",
       "      <td>WALKER</td>\n",
       "      <td>CA</td>\n",
       "      <td>6063.0</td>\n",
       "      <td>40.053250</td>\n",
       "      <td>-120.668900</td>\n",
       "      <td>2019-09-04</td>\n",
       "      <td>NaT</td>\n",
       "      <td>SUSANVILLE 2 SW, CA US</td>\n",
       "      <td>40.41670</td>\n",
       "      <td>-120.66310</td>\n",
       "      <td>2019-09-01</td>\n",
       "      <td>Missing data/not specified/undetermined</td>\n",
       "      <td>54608.0</td>\n",
       "      <td>G</td>\n",
       "      <td>1283.8</td>\n",
       "      <td>14.81</td>\n",
       "      <td>23.62</td>\n",
       "      <td>5.99</td>\n",
       "      <td>35.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144415</th>\n",
       "      <td>OK 745</td>\n",
       "      <td>AL</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>32.997230</td>\n",
       "      <td>-87.304390</td>\n",
       "      <td>2019-09-18</td>\n",
       "      <td>NaT</td>\n",
       "      <td>BANKHEAD LOCK AND DAM, AL US</td>\n",
       "      <td>33.45270</td>\n",
       "      <td>-87.35720</td>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>Missing data/not specified/undetermined</td>\n",
       "      <td>413.0</td>\n",
       "      <td>E</td>\n",
       "      <td>85.3</td>\n",
       "      <td>19.26</td>\n",
       "      <td>25.34</td>\n",
       "      <td>13.17</td>\n",
       "      <td>174.7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144416</th>\n",
       "      <td>204 COW</td>\n",
       "      <td>OR</td>\n",
       "      <td>41001.0</td>\n",
       "      <td>44.285050</td>\n",
       "      <td>-118.459800</td>\n",
       "      <td>2019-08-09</td>\n",
       "      <td>NaT</td>\n",
       "      <td>JOHN DAY, OR US</td>\n",
       "      <td>44.42330</td>\n",
       "      <td>-118.95940</td>\n",
       "      <td>2019-08-01</td>\n",
       "      <td>Natural</td>\n",
       "      <td>9668.0</td>\n",
       "      <td>G</td>\n",
       "      <td>933.6</td>\n",
       "      <td>19.26</td>\n",
       "      <td>29.83</td>\n",
       "      <td>8.69</td>\n",
       "      <td>8.7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                FIRE_NAME STATE  FIPS_CODE  FIRE_LATITUDE  FIRE_LONGITUDE  \\\n",
       "144407        BARREN HILL    ID    16049.0      46.236944     -114.982800   \n",
       "144408         SAN RAFAEL    AZ     4023.0      31.423040     -110.571000   \n",
       "144409               ROCK    CA     6099.0      37.472222     -121.249444   \n",
       "144410          CONNEX WF    FL    12091.0      30.523333      -86.781667   \n",
       "144411        BEAVER POND    MS    28153.0      31.493330      -88.740280   \n",
       "144412  BEN HOWARD HOLLOW    KY    21013.0      36.850278      -83.506667   \n",
       "144413          2019-3354    KS    20139.0      36.711640      -96.740750   \n",
       "144414             WALKER    CA     6063.0      40.053250     -120.668900   \n",
       "144415             OK 745    AL     1007.0      32.997230      -87.304390   \n",
       "144416            204 COW    OR    41001.0      44.285050     -118.459800   \n",
       "\n",
       "        FIRE_DATE CONTAIN_DATE                CLOSEST_STATION  STATION_LAT  \\\n",
       "144407 2019-07-01          NaT            SELWAY LODGE, ID US     46.00810   \n",
       "144408 2019-07-01          NaT  PATAGONIA PATON CENTER, AZ US     31.53923   \n",
       "144409 2019-06-25          NaT           TRACY CARBONA, CA US     37.68190   \n",
       "144410 2019-10-07          NaT               NICEVILLE, FL US     30.53160   \n",
       "144411 2019-06-13          NaT          WAYNESBORO 2 W, MS US     31.67730   \n",
       "144412 2019-09-21          NaT              HARLAN 3 S, KY US     36.80580   \n",
       "144413 2019-11-25          NaT                 RALSTON, OK US     36.50440   \n",
       "144414 2019-09-04          NaT         SUSANVILLE 2 SW, CA US     40.41670   \n",
       "144415 2019-09-18          NaT   BANKHEAD LOCK AND DAM, AL US     33.45270   \n",
       "144416 2019-08-09          NaT                JOHN DAY, OR US     44.42330   \n",
       "\n",
       "        STATION_LON READINGS_DATE                     CAUSE_CLASSIFICATION  \\\n",
       "144407   -114.84420    2019-07-01                                  Natural   \n",
       "144408   -110.76028    2019-07-01                                  Natural   \n",
       "144409   -121.34660    2019-07-01  Missing data/not specified/undetermined   \n",
       "144410    -86.49280    2019-10-01                                    Human   \n",
       "144411    -88.67090    2019-06-01                                    Human   \n",
       "144412    -83.34410    2019-10-01                                    Human   \n",
       "144413    -96.74380    2019-12-01                                    Human   \n",
       "144414   -120.66310    2019-09-01  Missing data/not specified/undetermined   \n",
       "144415    -87.35720    2019-10-01  Missing data/not specified/undetermined   \n",
       "144416   -118.95940    2019-08-01                                  Natural   \n",
       "\n",
       "        FIRE_SIZE FIRE_SIZE_CLASS  ELEVATION   TAVG   TMAX   TMIN   PRCP  \\\n",
       "144407     1592.0               F      786.4  19.28  31.05   7.51   23.4   \n",
       "144408      438.0               E     1232.6  25.41  35.49  15.34   65.8   \n",
       "144409     2422.0               F       41.1  24.63  34.24  15.02    0.0   \n",
       "144410      970.0               E       22.6  21.55  28.44  14.66  146.7   \n",
       "144411      168.0               D       61.0  26.21  32.35  20.06   91.8   \n",
       "144412      272.0               D      378.0  15.24  22.17   8.31  104.5   \n",
       "144413     1000.0               F      251.5   5.92  12.90  -1.06   10.2   \n",
       "144414    54608.0               G     1283.8  14.81  23.62   5.99   35.6   \n",
       "144415      413.0               E       85.3  19.26  25.34  13.17  174.7   \n",
       "144416     9668.0               G      933.6  19.26  29.83   8.69    8.7   \n",
       "\n",
       "        DAYS_TO_CONTAIN  \n",
       "144407                0  \n",
       "144408                0  \n",
       "144409                0  \n",
       "144410                0  \n",
       "144411                0  \n",
       "144412                0  \n",
       "144413                0  \n",
       "144414                0  \n",
       "144415                0  \n",
       "144416                0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Read the CSV file\n",
    "merged_results = pd.read_csv('Outputs/merged_results.csv')\n",
    "\n",
    "# Select specific columns from the DataFrame\n",
    "us_data_2018_test = merged_results[['FIRE_NAME', 'STATE', 'FIPS_CODE', 'LATITUDE', 'LONGITUDE', 'DISCOVERY_DATE', 'CONT_DATE', 'name', 'latitude', 'longitude', 'date',\n",
    "                                    'NWCG_CAUSE_CLASSIFICATION', 'FIRE_SIZE', 'FIRE_SIZE_CLASS',\n",
    "                                    'elevation', 'TAVG', 'TMAX', 'TMIN', 'PRCP']]\n",
    "\n",
    "# Rename the columns\n",
    "us_data_2018_test.columns = ['FIRE_NAME', 'STATE', 'FIPS_CODE', 'FIRE_LATITUDE', 'FIRE_LONGITUDE', 'FIRE_DATE', 'CONTAIN_DATE', 'CLOSEST_STATION', 'STATION_LAT', 'STATION_LON', 'READINGS_DATE',\n",
    "                             'CAUSE_CLASSIFICATION', 'FIRE_SIZE', 'FIRE_SIZE_CLASS',\n",
    "                             'ELEVATION', 'TAVG', 'TMAX', 'TMIN', 'PRCP']\n",
    "\n",
    "# Convert 'CONTAIN_DATE' and 'FIRE_DATE' columns to datetime\n",
    "us_data_2018_test['CONTAIN_DATE'] = pd.to_datetime(us_data_2018_test['CONTAIN_DATE'])\n",
    "us_data_2018_test['FIRE_DATE'] = pd.to_datetime(us_data_2018_test['FIRE_DATE'])\n",
    "\n",
    "# Calculate the difference between 'CONTAIN_DATE' and 'FIRE_DATE'\n",
    "us_data_2018_test['DAYS_TO_CONTAIN'] = (us_data_2018_test['CONTAIN_DATE'] - us_data_2018_test['FIRE_DATE']).dt.days\n",
    "\n",
    "# Replace any NaN values in 'DAYS_TO_CONTAIN' with 1\n",
    "us_data_2018_test['DAYS_TO_CONTAIN'].fillna(0, inplace=True)\n",
    "\n",
    "# Define float and int columns\n",
    "float_columns = {\n",
    "    'FIRE_LATITUDE': float,\n",
    "    'FIRE_LONGITUDE': float,\n",
    "    'STATION_LAT': float,\n",
    "    'STATION_LON': float,\n",
    "    'FIRE_SIZE': float,\n",
    "    'ELEVATION': float,\n",
    "    'TAVG': float,\n",
    "    'TMAX': float,\n",
    "    'TMIN': float,\n",
    "    'PRCP': float\n",
    "}\n",
    "\n",
    "int_columns = {\n",
    "    'DAYS_TO_CONTAIN': int,\n",
    "}\n",
    "\n",
    "# Convert columns to the specified data types\n",
    "us_data_2018_test = us_data_2018_test.astype({**float_columns, **int_columns})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_filename = 'Outputs/us_data_2018_2019.csv'\n",
    "us_data_2018_test.to_csv(csv_filename, index=False)\n",
    "\n",
    "# Display the last 20 rows of the DataFrame\n",
    "us_data_2018_test.tail(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "us_data_2018 = merged_results[['FIRE_NAME', 'STATE', 'FIPS_CODE', 'LATITUDE', 'LONGITUDE', 'DISCOVERY_DATE', 'CONT_DATE', 'name', 'latitude', 'longitude', 'date',\n",
    "                                    'NWCG_CAUSE_CLASSIFICATION', 'FIRE_SIZE', 'FIRE_SIZE_CLASS',\n",
    "                                    'elevation', 'TAVG', 'TMAX', 'TMIN']]\n",
    "\n",
    "# Rename the columns\n",
    "us_data_2018.columns = ['FIRE_NAME', 'STATE', 'FIPS_CODE', 'FIRE_LATITUDE', 'FIRE_LONGITUDE', 'FIRE_DATE', 'CONTAIN_DATE', 'CLOSEST_STATION', 'STATION_LAT', 'STATION_LON', 'READINGS_DATE',\n",
    "                               'CAUSE_CLASSIFICATION', 'FIRE_SIZE', 'FIRE_SIZE_CLASS',\n",
    "                             'ELEVATION', 'TAVG', 'TMAX', 'TMIN']\n",
    "\n",
    "\n",
    "# Calculate the difference between 'CONTAIN_DATE' and 'FIRE_DATE'\n",
    "us_data_2018['CONTAIN_DATE'] = pd.to_datetime(us_data_2018['CONTAIN_DATE'])\n",
    "us_data_2018['DAYS_TO_CONTAIN'] = (us_data_2018['CONTAIN_DATE'] - us_data_2018['FIRE_DATE']).dt.days\n",
    "\n",
    "# Replace any NaN values in 'DAYS_TO_CONTAIN' with 1\n",
    "us_data_2018['DAYS_TO_CONTAIN'].fillna(0, inplace=True)\n",
    "\n",
    "float_columns = {\n",
    "    'FIRE_LATITUDE': float,\n",
    "    'FIRE_LONGITUDE': float,\n",
    "    'STATION_LAT': float,\n",
    "    'STATION_LON': float,\n",
    "    'FIRE_SIZE': float,\n",
    "    'ELEVATION': float,\n",
    "    'TAVG': float,\n",
    "    'TMAX': float,\n",
    "    'TMIN': float\n",
    "}\n",
    "\n",
    "int_columns = {\n",
    "    'DAYS_TO_CONTAIN': int,\n",
    "}\n",
    "\n",
    "us_data_2018 = us_data_2018.astype({**float_columns, **int_columns})\n",
    "\n",
    "csv_filename = 'Outputs/us_data_2018_2019.csv'\n",
    "us_data_2018.to_csv(csv_filename, index=False)\n",
    "\n",
    "us_data_2018.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25104\\2374848524.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# Iterate over each row in wildfires\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwildfires\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mnearest_row\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_nearest_match\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrouped_df_detailed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'date'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'latitude'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'longitude'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# Add a 'fire?' column to the nearest_row DataFrame and set it to 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25104\\2374848524.py\u001b[0m in \u001b[0;36mfind_nearest_match\u001b[1;34m(row, df, date_col, lat_col, lon_col)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Define the find_nearest_match function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfind_nearest_match\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdate_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlat_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlon_col\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mdate_diff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdate_col\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DISCOVERY_DATE'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mlat_diff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlat_col\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'LATITUDE'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mlon_diff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlon_col\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'LONGITUDE'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kentr\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\common.py\u001b[0m in \u001b[0;36mnew_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kentr\\anaconda3\\lib\\site-packages\\pandas\\core\\arraylike.py\u001b[0m in \u001b[0;36m__sub__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"__sub__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__sub__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_arith_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"__rsub__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kentr\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   6110\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_arith_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6111\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malign_method_SERIES\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6112\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIndexOpsMixin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_arith_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kentr\\anaconda3\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m_arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   1346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marithmetic_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kentr\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001b[0m in \u001b[0;36marithmetic_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;31m# Timedelta/Timestamp and other custom scalars are included in the check\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;31m# because numexpr will fail on it, see GH#31457\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mres_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[1;31m# TODO we should handle EAs consistently and move this check before the if/else\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kentr\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\common.py\u001b[0m in \u001b[0;36mnew_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kentr\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\datetimelike.py\u001b[0m in \u001b[0;36m__sub__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   1383\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_offset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1385\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sub_datetimelike_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1386\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1387\u001b[0m             \u001b[1;31m# This check must come after the check for np.timedelta64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kentr\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\datetimelike.py\u001b[0m in \u001b[0;36m_sub_datetimelike_scalar\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   1096\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_matching_resos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sub_datetimelike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kentr\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\datetimelike.py\u001b[0m in \u001b[0;36m_sub_datetimelike\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   1124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1125\u001b[0m         \u001b[0mother_i8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_i8_values_and_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1126\u001b[1;33m         res_values = checked_add_with_arr(\n\u001b[0m\u001b[0;32m   1127\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masi8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mother_i8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_isnan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mo_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\kentr\\anaconda3\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36mchecked_add_with_arr\u001b[1;34m(arr, b, arr_mask, b_mask)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0marr_mask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mb2_mask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1155\u001b[1;33m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mputmask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m~\u001b[0m\u001b[0mnot_nan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miNaT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1157\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kentr\\anaconda3\\lib\\site-packages\\numpy\\core\\overrides.py\u001b[0m in \u001b[0;36mputmask\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Read your data\n",
    "wildfires = pd.read_csv('Resources/data.csv')\n",
    "grouped_df_detailed = pd.read_csv('Outputs/grouped_df_detailed.csv')\n",
    "\n",
    "# Convert date columns to datetime\n",
    "wildfires['DISCOVERY_DATE'] = pd.to_datetime(wildfires['DISCOVERY_DATE'])\n",
    "grouped_df_detailed['date'] = pd.to_datetime(grouped_df_detailed['date'])\n",
    "\n",
    "# Define the find_nearest_match function\n",
    "def find_nearest_match(row, df, date_col, lat_col, lon_col):\n",
    "    date_diff = abs((df[date_col] - row['DISCOVERY_DATE']).dt.total_seconds())\n",
    "    lat_diff = abs(df[lat_col] - row['LATITUDE'])\n",
    "    lon_diff = abs(df[lon_col] - row['LONGITUDE'])\n",
    "    total_diff = date_diff + lat_diff + lon_diff\n",
    "    nearest_idx = total_diff.idxmin()\n",
    "    return df.loc[nearest_idx]\n",
    "\n",
    "# Create a list to hold the merged rows\n",
    "merged_rows = []\n",
    "\n",
    "# Iterate over each row in wildfires\n",
    "for idx, row in wildfires.iterrows():\n",
    "    nearest_row = find_nearest_match(row, grouped_df_detailed, 'date', 'latitude', 'longitude')\n",
    "    \n",
    "    # Add a 'fire?' column to the nearest_row DataFrame and set it to 1\n",
    "    nearest_row['fire?'] = 1\n",
    "    \n",
    "    # Combine the original wildfire row with the matched nearest_row\n",
    "    merged_row = pd.concat([row, nearest_row])\n",
    "    \n",
    "    # Append the merged row to merged_rows list\n",
    "    merged_rows.append(merged_row)\n",
    "\n",
    "# Create a temporary DataFrame to hold unmatched rows\n",
    "unmatched_rows = []\n",
    "\n",
    "# Create the merged_results_test DataFrame from the list of merged rows\n",
    "merged_results_test = pd.concat(merged_rows, axis=1).T\n",
    "\n",
    "# Iterate over each row in grouped_df_detailed\n",
    "for idx, row in grouped_df_detailed.iterrows():\n",
    "    if idx not in merged_results_test.index:\n",
    "        # Add a 'fire?' column to the row and set it to 0\n",
    "        row['fire?'] = 0\n",
    "        unmatched_rows.append(row)\n",
    "\n",
    "# Concatenate unmatched_rows with merged_rows\n",
    "merged_rows += unmatched_rows\n",
    "\n",
    "# Reorder columns\n",
    "merged_results_test = merged_results_test[['LATITUDE', 'LONGITUDE','COUNTY', 'FIPS_CODE','FIRE_SIZE', 'FIRE_SIZE_CLASS',\n",
    "       'NWCG_CAUSE_CLASSIFICATION', 'NWCG_GENERAL_CAUSE', 'FIRE_NAME',\n",
    "       'DISCOVERY_DATE', 'CONT_DATE', 'DISCOVERY_TIME', 'CONT_TIME', 'STATE',\n",
    "       'station', 'name', 'latitude', 'longitude', 'elevation', 'date', 'TAVG',\n",
    "       'TMAX', 'TMIN','PRCP', 'fire?']]\n",
    "\n",
    "# Save to CSV\n",
    "csv_filename = 'Outputs/merged_results_test.csv'\n",
    "merged_results_test.to_csv(csv_filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>FIPS_CODE</th>\n",
       "      <th>FIRE_SIZE</th>\n",
       "      <th>FIRE_SIZE_CLASS</th>\n",
       "      <th>NWCG_CAUSE_CLASSIFICATION</th>\n",
       "      <th>NWCG_GENERAL_CAUSE</th>\n",
       "      <th>FIRE_NAME</th>\n",
       "      <th>DISCOVERY_DATE</th>\n",
       "      <th>...</th>\n",
       "      <th>station</th>\n",
       "      <th>name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>elevation</th>\n",
       "      <th>date</th>\n",
       "      <th>TAVG</th>\n",
       "      <th>TMAX</th>\n",
       "      <th>TMIN</th>\n",
       "      <th>PRCP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46.275833</td>\n",
       "      <td>-114.379167</td>\n",
       "      <td>081</td>\n",
       "      <td>30081.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>A</td>\n",
       "      <td>Natural</td>\n",
       "      <td>Natural</td>\n",
       "      <td>BLODGETT</td>\n",
       "      <td>2018-08-22</td>\n",
       "      <td>...</td>\n",
       "      <td>GHCND:USC00243885</td>\n",
       "      <td>HAMILTON, MT US</td>\n",
       "      <td>46.24622</td>\n",
       "      <td>-114.16794</td>\n",
       "      <td>1092.7</td>\n",
       "      <td>2018-09-01</td>\n",
       "      <td>12.89</td>\n",
       "      <td>21.95</td>\n",
       "      <td>3.82</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46.404167</td>\n",
       "      <td>-113.921944</td>\n",
       "      <td>081</td>\n",
       "      <td>30081.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>A</td>\n",
       "      <td>Human</td>\n",
       "      <td>Equipment and vehicle use</td>\n",
       "      <td>CORLEY GULCH</td>\n",
       "      <td>2018-07-26</td>\n",
       "      <td>...</td>\n",
       "      <td>GHCND:USC00247894</td>\n",
       "      <td>STEVENSVILLE, MT US</td>\n",
       "      <td>46.5137</td>\n",
       "      <td>-114.091</td>\n",
       "      <td>1028.7</td>\n",
       "      <td>2018-08-01</td>\n",
       "      <td>18.29</td>\n",
       "      <td>28.05</td>\n",
       "      <td>8.53</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46.245833</td>\n",
       "      <td>-114.308889</td>\n",
       "      <td>081</td>\n",
       "      <td>30081.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>B</td>\n",
       "      <td>Human</td>\n",
       "      <td>Recreation and ceremony</td>\n",
       "      <td>CANYON CREEK</td>\n",
       "      <td>2018-09-21</td>\n",
       "      <td>...</td>\n",
       "      <td>GHCND:USC00243885</td>\n",
       "      <td>HAMILTON, MT US</td>\n",
       "      <td>46.24622</td>\n",
       "      <td>-114.16794</td>\n",
       "      <td>1092.7</td>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>6.96</td>\n",
       "      <td>13.28</td>\n",
       "      <td>0.65</td>\n",
       "      <td>33.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45.784722</td>\n",
       "      <td>-114.033056</td>\n",
       "      <td>081</td>\n",
       "      <td>30081.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>A</td>\n",
       "      <td>Natural</td>\n",
       "      <td>Natural</td>\n",
       "      <td>MAYNARD CREEK</td>\n",
       "      <td>2018-08-17</td>\n",
       "      <td>...</td>\n",
       "      <td>GHCND:USC00242221</td>\n",
       "      <td>DARBY, MT US</td>\n",
       "      <td>46.0263</td>\n",
       "      <td>-114.1763</td>\n",
       "      <td>1182.6</td>\n",
       "      <td>2018-09-01</td>\n",
       "      <td>11.77</td>\n",
       "      <td>20.79</td>\n",
       "      <td>2.74</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45.986944</td>\n",
       "      <td>-113.807222</td>\n",
       "      <td>081</td>\n",
       "      <td>30081.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>A</td>\n",
       "      <td>Natural</td>\n",
       "      <td>Natural</td>\n",
       "      <td>BLUE</td>\n",
       "      <td>2018-08-12</td>\n",
       "      <td>...</td>\n",
       "      <td>GHCND:USC00247967</td>\n",
       "      <td>SULA 14 NE, MT US</td>\n",
       "      <td>45.911</td>\n",
       "      <td>-113.7394</td>\n",
       "      <td>1571.2</td>\n",
       "      <td>2018-08-01</td>\n",
       "      <td>15.1</td>\n",
       "      <td>26.09</td>\n",
       "      <td>4.12</td>\n",
       "      <td>19.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    LATITUDE   LONGITUDE COUNTY FIPS_CODE FIRE_SIZE FIRE_SIZE_CLASS  \\\n",
       "0  46.275833 -114.379167    081   30081.0       0.1               A   \n",
       "1  46.404167 -113.921944    081   30081.0       0.1               A   \n",
       "2  46.245833 -114.308889    081   30081.0       1.0               B   \n",
       "3  45.784722 -114.033056    081   30081.0       0.1               A   \n",
       "4  45.986944 -113.807222    081   30081.0       0.1               A   \n",
       "\n",
       "  NWCG_CAUSE_CLASSIFICATION         NWCG_GENERAL_CAUSE      FIRE_NAME  \\\n",
       "0                   Natural                    Natural       BLODGETT   \n",
       "1                     Human  Equipment and vehicle use   CORLEY GULCH   \n",
       "2                     Human    Recreation and ceremony   CANYON CREEK   \n",
       "3                   Natural                    Natural  MAYNARD CREEK   \n",
       "4                   Natural                    Natural           BLUE   \n",
       "\n",
       "  DISCOVERY_DATE  ...            station                 name  latitude  \\\n",
       "0     2018-08-22  ...  GHCND:USC00243885      HAMILTON, MT US  46.24622   \n",
       "1     2018-07-26  ...  GHCND:USC00247894  STEVENSVILLE, MT US   46.5137   \n",
       "2     2018-09-21  ...  GHCND:USC00243885      HAMILTON, MT US  46.24622   \n",
       "3     2018-08-17  ...  GHCND:USC00242221         DARBY, MT US   46.0263   \n",
       "4     2018-08-12  ...  GHCND:USC00247967    SULA 14 NE, MT US    45.911   \n",
       "\n",
       "   longitude elevation       date   TAVG   TMAX  TMIN  PRCP  \n",
       "0 -114.16794    1092.7 2018-09-01  12.89  21.95  3.82   3.5  \n",
       "1   -114.091    1028.7 2018-08-01  18.29  28.05  8.53   8.6  \n",
       "2 -114.16794    1092.7 2018-10-01   6.96  13.28  0.65  33.7  \n",
       "3  -114.1763    1182.6 2018-09-01  11.77  20.79  2.74   9.1  \n",
       "4  -113.7394    1571.2 2018-08-01   15.1  26.09  4.12  19.4  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_results_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['LATITUDE', 'LONGITUDE', 'COUNTY', 'FIPS_CODE', 'FIRE_SIZE',\n",
       "       'FIRE_SIZE_CLASS', 'NWCG_CAUSE_CLASSIFICATION', 'NWCG_GENERAL_CAUSE',\n",
       "       'FIRE_NAME', 'DISCOVERY_DATE', 'CONT_DATE', 'DISCOVERY_TIME',\n",
       "       'CONT_TIME', 'STATE', 'station', 'name', 'latitude', 'longitude',\n",
       "       'elevation', 'date', 'TAVG', 'TMAX', 'TMIN', 'PRCP'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_results_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144417, 24)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_results_test.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
